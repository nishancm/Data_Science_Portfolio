{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is gradient  descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a numerical method used to iteratively find parameters that minimize a given function of interest. \n",
    "\n",
    "For e.g. Lets say we expect $y = ax^2 +bx +c$ where $x$ is independent variable and $y$ is the dependent variable. Here $a$, $b$, and $c$ are parameter values. So we might want to find the parameter values such that fitted value of y based on the parameters are close to the actual y values as possible. Therefore the loss function associated with in this case can be some form of function giving the error between the fitted and the actual values (e.g. Mean Squared Error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use gradient descent in machine learning\n",
    "\n",
    "In machine learning we have the objective of finding the optimum parameters that minimize the loss function we are interested in\n",
    "\n",
    "Some loss functions (e.g. Mean Squared Error) has analytical solutions to derive the optimum parameters. However, some loss functions (e.g. cross entropy) does not have analytical solutions. Gradient descent offers an effective way of iteratively reaching towards the optimum parameters of any loss function which is differentiable. And using gradient descent is much faster particularly for large datasets by avoid using computationally expensive operations such as matrix inversions attached with analytical solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest version of  gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ w_t = w_{t-1} - \\eta\\nabla_w L(w_t, x_{i_j}, y_{i_j}) $\n",
    "\n",
    "$\\nabla_w L(w_t, x_{i_j}, y_{i_j})$ gives you the derivative of the loss function at the current(previous) estimates of the parameters. Negative of this indicate the direction of the slope and the magnitude. Therefore we step downhill by subtracting gradient of loss function from the previous estimate of parameters to find the new estimate of the parameters. In this process, $\\eta$ helps to control our step (subtraction) and it is called the learning rate.\n",
    "\n",
    "We do this process iteratively until our next estimate and the previous estimate are very close (this can be defined as a difference). And we have to pick an initial value for the parameters (e.g all 0) to before starting the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main drawback of the simplest form mentioned above is that calculation of the gradient for loss function involves whole dataset at each iteration. So for very large datasets this can become very inefficient. So the idea of Stochastic gradient descent is to use only one randomly selected observation from the data to calculate the derivative. This makes Stochastic gradient descent much faster. One other benefit of this approach is, it helps online learning as learning can be done with each observed value.\n",
    "\n",
    "However the drawback with this approach is that the loss is minimized in a very noisy manner (with many oscillations). Yet with enough iterations stochastic gradient descent will help to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini batch gradient descent finds a middle ground between stochastic gradient descent and simplest version of gradient descent by taking a random sample from the dataset to derive gradient of the loss function. Thus result in less oscillations for the loss value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several extensions for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum helps to reduce the oscillations (oscillations can be thought of as digressions from the path the loss function should take) in the loss and helps to converge faster. Nestrov momentum is an enhanced version that facilitate faster and better convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad - This partially helps the selection process of the learning rate. Here the learning rate reduce over the iterations to facilitate a better convergence. This particularly helps when the data is sparse (e.g. bag of words representations). However, reduction of the learning rate way to soon might result in earlier stops of the algorithm. \n",
    "\n",
    "RMSprop - RMSprop overcome the issue of AdaGrad and helps offer better control over the change in learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam combines both extensions in momentum and adaptive learning rate in its algorithm and hence considered as a superior gradient descent approach, particularly used in deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference - https://www.coursera.org/learn/intro-to-deep-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
