{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets first get all the URLs for matches from 2009-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_season_url(year):\n",
    "    driver = webdriver.Chrome()\n",
    "    get_all_fixtures(driver, year) # make we are the landing page to select the week\n",
    "    time.sleep(5) # make sure webpage is loaded\n",
    "    \n",
    "    season_urls = {}\n",
    "    \n",
    "    for w in range(20): # 20 make sure to get all the rounds \n",
    "        urls = get_week_urls(driver, w+1, year)\n",
    "        if len(urls)>0:\n",
    "            season_urls['week_%s' % str(w+1)] = urls\n",
    "            print('week '+str(w+1)+' urls extracted')\n",
    "    \n",
    "    driver.close()\n",
    "    write_data(season_urls, 'Data/URLS/', year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_fixtures(driver, year):\n",
    "    main_page = 'https://sanzarrugby.com/superrugby/fixtures/archives/'\n",
    "    driver.get(main_page) # go to the archives page\n",
    "    \n",
    "    link_season = \"/superrugby/fixtures/archives/%s-super-rugby/\" % year\n",
    "    season = driver.find_element_by_xpath('//a[@href=\"'+link_season+'\"]')\n",
    "    season.click() # go to season page\n",
    "    \n",
    "    link_fixtures = \"#fixtures\"\n",
    "    fixtures = driver.find_element_by_xpath('//a[@href=\"'+link_fixtures+'\"]')\n",
    "    fixtures.click() # go to all matches for the season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_week_urls(driver, week, year):\n",
    "    select_week(driver, week)\n",
    "    \n",
    "    return get_match_urls(driver, week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_week(driver, week):\n",
    "    round_filter = \"roundFilter\"\n",
    "    round_menu = driver.find_element_by_id(round_filter)\n",
    "    round_menu.click() # get the drop down meanu to select the week\n",
    "    \n",
    "    link_round = \"#round%s\" % week\n",
    "        \n",
    "    try:\n",
    "        round_  = driver.find_element_by_xpath('//a[@href=\"'+link_round+'\"]')\n",
    "        print(round_.text)\n",
    "        if ('Week' not in round_.text) & ('Round' not in round_.text): # we are interested only in league stage for now\n",
    "            return None\n",
    "        round_.click() # go to the week\n",
    "    except:\n",
    "        pass # week number is out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_match_urls(driver, week):\n",
    "    # get match links\n",
    "    link_path = '//*[@id=\"Opta_%s\"]/div/div/div/table/tbody[%s]/tr/td[9]/a'\n",
    "    match_links = []\n",
    "    for i in range(20): # 20 makesure to capture all the rows of the table with fixtures\n",
    "        ln_p = link_path % (str(week), str(i+1)) #xpath for the match\n",
    "        \n",
    "        try:\n",
    "            ln =  driver.find_element_by_xpath(ln_p)\n",
    "            match_links.append(ln.get_attribute('href'))\n",
    "        except:\n",
    "            pass # element not found for out of range number of rows int he table\n",
    "\n",
    "    return match_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_data(data, filepath, year):\n",
    "    with open(filepath + year + '.json', 'w') as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all urls\n",
    "for y in ['2017','2016','2015','2014','2013','2012']:\n",
    "    get_season_url(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets now get data for matches using URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_season(year):\n",
    "    \n",
    "    COMPLETE =  False\n",
    "    CURRENT_WEEK = 0\n",
    "    CURRENT_MATCH = 0\n",
    "    season = {}\n",
    "    while not COMPLETE:\n",
    "        CLOSE = 0\n",
    "        driver = webdriver.Chrome()\n",
    "\n",
    "        urls = get_json_data('Data/URLS/'+year+'.json')\n",
    "        \n",
    "        match_variables = get_json_data('match_variables.json')\n",
    "            \n",
    "        for k,v in match_variables.items():\n",
    "            match_variables[k] = v.replace(\"'\", '\"') # reverse changes made for varible have valid JSON form\n",
    "\n",
    "        \n",
    "        BREAK = False\n",
    "        while not COMPLETE:\n",
    "            for i, week in enumerate(urls):\n",
    "\n",
    "                if i < CURRENT_WEEK: # make sure weeks before are not repeated\n",
    "                    continue\n",
    "\n",
    "                print('Data extraction started for week_%s' % str(i+1))\n",
    "\n",
    "                matches = {}\n",
    "                for j, u in enumerate(urls[week]): # one url at a time \n",
    "                    \n",
    "                    if (i <= CURRENT_WEEK) & (j < CURRENT_MATCH): \n",
    "                        # make sure previous matches are not repeated\n",
    "                        continue\n",
    "                        \n",
    "                    # special case website not responding\n",
    "                    if (year == '2013') & (i==6) & (j==6):\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        matches['match_%s' % str(j+1)] = get_match_data(driver, u, \n",
    "                                                                        match_variables)\n",
    "                        print('Data extracted match_%s' % str(j+1))\n",
    "                    except:\n",
    "                        BREAK = True\n",
    "                        CURRENT_WEEK = i\n",
    "                        CURRENT_MATCH = j\n",
    "                        break\n",
    "\n",
    "                try: # make all the matches scraped so far is stored\n",
    "                    season['week_%s' % str(i+1)] = {**season['week_%s' % str(i+1)], \n",
    "                                                    **matches}\n",
    "                except:\n",
    "                    season['week_%s' % str(i+1)] = matches\n",
    "\n",
    "                if BREAK:\n",
    "                    break\n",
    "\n",
    "            if BREAK: \n",
    "                BREAK = False\n",
    "                CLOSE += 1\n",
    "                if CLOSE == 5: break\n",
    "            else: COMPLETE = True\n",
    "\n",
    "        driver.close()       \n",
    "    write_data(season, 'Data/Match_info/', year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_json_data(filename):\n",
    "    with open(filename, 'r') as js_data:\n",
    "        return json.load(js_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_match_data(driver, url, match_variables):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    match = {}\n",
    "\n",
    "    for k,v in match_variables.items():\n",
    "        match[k] = driver.find_element_by_xpath(v).text\n",
    "        \n",
    "    match = get_possession_info(driver, match) # possession data has a different structure\n",
    "\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_possession_info(driver, match):\n",
    "    pos = driver.find_elements_by_class_name('Opta-Territories-value')\n",
    "    \n",
    "    var = [\"team1_25\",\"team2_op_25\",\"team1_40\",\"team2_op_40\", \n",
    "           \"team1_op_40\",\"team2_40\",\"team1_op_25\",\"team2_25\"]\n",
    "    \n",
    "    for i,p in enumerate(pos):\n",
    "        match[var[i]] = p.text\n",
    "        \n",
    "    match[\"overall_pos1\"] = driver\\\n",
    "                .find_element_by_css_selector(\".Opta-Possession-value.Opta-Home\").text\n",
    "    match[\"overall_pos2\"] = driver\\\n",
    "                .find_element_by_css_selector(\".Opta-Possession-value.Opta-Away\").text\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get seasons\n",
    "for y in ['2017','2016','2015','2014', '2013','2012']:\n",
    "    get_season(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
